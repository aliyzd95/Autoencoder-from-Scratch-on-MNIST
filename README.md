# Autoencoder from Scratch on MNIST

This project implements a simple feedforward autoencoder from scratch using NumPy, trained on the MNIST dataset. The model compresses the input images into a lower-dimensional representation and reconstructs them back. It supports different activation functions (`sigmoid`, `tanh`) and uses mini-batch gradient descent for optimization.

## Key features:
- Fully customizable architecture and hyperparameters
- Manual implementation of forward and backward passes
- Early stopping and validation monitoring
- Visualization of original vs reconstructed images
- Extraction of encoded features for downstream tasks

This project is ideal for educational purposes and for those interested in understanding the inner workings of autoencoders without relying on high-level deep learning libraries.
